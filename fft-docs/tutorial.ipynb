{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0167d15-a6ff-4602-835d-c88851e83113",
   "metadata": {},
   "source": [
    "# Meta Motivo Tutorial\n",
    "This notebook provides a simple introduction on how to use the Meta Motivo api."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be0d8e4a-882c-467e-bce6-ef2f33b509e2",
   "metadata": {},
   "source": [
    "## All imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d3cd63-1d2e-4bda-b224-d2b2b73bf655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from metamotivo.fb_cpr.huggingface import FBcprModel\n",
    "from huggingface_hub import hf_hub_download\n",
    "from humenv import make_humenv\n",
    "from gymnasium.wrappers import FlattenObservation, TransformObservation\n",
    "from metamotivo.buffers.buffers import DictBuffer\n",
    "from humenv.env import make_from_name\n",
    "from humenv import rewards as humenv_rewards\n",
    "\n",
    "import torch\n",
    "import mediapy as media\n",
    "import math\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa35d241-fa2d-4ad3-aaa9-9e2b4175e742",
   "metadata": {},
   "source": [
    "## Model download\n",
    "The first step is to download the model. We show how to use HuggingFace hub for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00f7b632-864d-4b05-848c-f7a22b662a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "embedding size torch.Size([1, 256])\n",
      "z norm: 15.999999046325684\n",
      "z norm / sqrt(d): 0.9999999403953552\n"
     ]
    }
   ],
   "source": [
    "# FB-CPR (Forward-Backward representations with Conditional Policy Regularization)\n",
    "model = FBcprModel.from_pretrained(\"/workspace/metamotivo/weights/metamotivo-M-1\")\n",
    "# print(model)\n",
    "device = \"cpu\"\n",
    "env, _ = make_humenv(\n",
    "    num_envs=1,\n",
    "    wrappers=[\n",
    "        FlattenObservation,\n",
    "        lambda env: TransformObservation(\n",
    "            env, lambda obs: torch.tensor(obs.reshape(1, -1), dtype=torch.float32, device=device), None\n",
    "        ),\n",
    "    ],\n",
    "    state_init=\"Default\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "z = model.sample_z(1)\n",
    "print(f\"embedding size {z.shape}\")\n",
    "print(f\"z norm: {torch.norm(z)}\")\n",
    "print(f\"z norm / sqrt(d): {torch.norm(z) / math.sqrt(z.shape[-1])}\")\n",
    "observation, _ = env.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3609f0e1-2ff9-462b-9bea-15695a128da7",
   "metadata": {},
   "source": [
    "**Run a policy from Meta Motivo:**\n",
    "\n",
    "Now that we saw how to load a pre-trained Meta Motivo policy, we can prompt it and execute actions with it. \n",
    "\n",
    "The first step is to sample a context embedding `z` that needs to be passed to the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f4e85ca-1940-403b-adfd-126c244e39ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# frames = [env.render()]\n",
    "for i in range(10):\n",
    "    action = model.act(observation, z, mean=True)\n",
    "    observation, reward, terminated, truncated, info = env.step(action.cpu().numpy().ravel())\n",
    "    # frames.append(env.render())\n",
    "    print()\n",
    "\n",
    "# media.show_video(frames, fps=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61d575bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 0.9035,  0.0891, -0.5720, -0.6155,  0.9966,  0.9993,  0.1562,  1.0000,\n",
      "          0.9818,  0.5814,  0.9950,  1.0000,  0.2559,  0.0516,  0.3607, -0.9981,\n",
      "          1.0000, -0.9619, -0.6218, -0.7449,  0.9997,  0.7061, -1.0000,  1.0000,\n",
      "         -0.9929,  0.0726,  0.1072, -0.2759, -0.1077,  0.4683, -0.6187, -0.0305,\n",
      "          0.2818, -0.2209, -0.1873,  0.6845, -1.0000,  0.9999,  0.9444, -1.0000,\n",
      "         -0.4481, -0.6869, -0.1910, -0.6637, -0.6141, -1.0000,  0.9961,  1.0000,\n",
      "         -0.9733,  0.9992, -1.0000,  1.0000, -0.2173, -0.3821, -1.0000,  1.0000,\n",
      "          0.9992, -0.1799,  0.6509,  0.9959,  0.9732, -0.9979, -1.0000,  1.0000,\n",
      "          0.2384, -1.0000, -1.0000, -0.0085, -0.3907]])\n"
     ]
    }
   ],
   "source": [
    "print(type(action))\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b09225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[ 9.0288e-01, -3.5818e-02,  7.4090e-02, -8.0363e-02,  8.9125e-02,\n",
      "          1.2323e-01, -4.3242e-01,  1.1802e-01,  1.1960e-01, -8.3199e-01,\n",
      "          2.1010e-01,  2.0011e-01, -8.8750e-01, -3.5517e-02, -6.2976e-02,\n",
      "         -8.6975e-02, -9.8935e-02, -1.4523e-01, -4.5730e-01, -2.2997e-01,\n",
      "         -2.0895e-01, -8.3084e-01, -1.0646e-01, -2.4749e-01, -8.6852e-01,\n",
      "          1.1301e-02, -8.0794e-03,  1.1139e-01,  2.3131e-02,  1.1292e-02,\n",
      "          2.4479e-01,  6.0650e-02,  2.1195e-02,  2.8883e-01,  6.6724e-02,\n",
      "          4.7383e-02,  5.0535e-01,  1.3728e-01,  7.1345e-02,  5.4184e-01,\n",
      "          4.0714e-02,  1.1323e-01,  4.0423e-01,  8.6296e-03,  2.0259e-01,\n",
      "          3.8764e-01, -3.9156e-02,  2.0912e-01,  1.3077e-01, -1.2484e-01,\n",
      "          2.0344e-01, -1.0335e-01, -1.5902e-01,  1.6669e-01, -1.7282e-01,\n",
      "          6.3904e-02, -4.4696e-02,  4.2278e-01,  1.0551e-01, -1.3759e-01,\n",
      "          4.2391e-01,  1.3916e-01, -1.0366e-01,  1.7348e-01,  1.8823e-01,\n",
      "         -1.3822e-01, -7.4849e-02,  2.2039e-01, -1.7093e-01, -1.4693e-01,\n",
      "          1.7173e-02,  9.9869e-01,  4.8253e-02,  9.4212e-01, -1.7764e-15,\n",
      "         -3.3529e-01, -5.3366e-02,  9.9831e-01,  2.2963e-02,  9.3556e-01,\n",
      "          4.1946e-02,  3.5066e-01, -1.6774e-01,  9.8575e-01,  1.2961e-02,\n",
      "          9.7046e-01,  1.6280e-01,  1.7807e-01, -4.8180e-01,  8.7627e-01,\n",
      "         -3.7946e-03,  8.7627e-01,  4.8181e-01,  3.2350e-03, -5.5615e-01,\n",
      "          8.3108e-01,  1.8540e-03,  8.3087e-01,  5.5596e-01,  2.3986e-02,\n",
      "          1.6049e-02,  9.9267e-01, -1.1982e-01,  9.8986e-01, -3.2687e-02,\n",
      "         -1.3822e-01, -8.5111e-02,  9.8005e-01, -1.7963e-01,  9.7123e-01,\n",
      "          4.1360e-02, -2.3452e-01,  7.5782e-02,  9.9279e-01, -9.2896e-02,\n",
      "          9.9555e-01, -7.0108e-02,  6.2900e-02,  1.5568e-01,  9.8765e-01,\n",
      "         -1.7867e-02,  9.8694e-01, -1.5628e-01, -3.9116e-02, -1.1301e-01,\n",
      "          9.8929e-01, -9.2406e-02,  9.9003e-01,  1.0425e-01, -9.4690e-02,\n",
      "         -1.1284e-01,  9.8932e-01, -9.2264e-02,  9.6144e-01,  8.5275e-02,\n",
      "         -2.6147e-01, -1.7524e-01,  9.7642e-01, -1.2611e-01,  9.5994e-01,\n",
      "          1.4101e-01, -2.4212e-01, -2.5259e-01,  9.6756e-01, -4.4921e-03,\n",
      "          8.7341e-01,  2.2601e-01, -4.3136e-01, -3.7069e-01,  8.7721e-01,\n",
      "          3.0511e-01,  8.2895e-01,  4.6064e-01, -3.1726e-01, -2.5909e-01,\n",
      "          8.3529e-01, -4.8494e-01,  9.6585e-01,  2.2256e-01, -1.3267e-01,\n",
      "         -6.8079e-02,  3.7079e-02, -9.9699e-01,  9.4191e-01, -3.2706e-01,\n",
      "         -7.6481e-02, -3.3897e-01, -5.8905e-02, -9.3895e-01,  9.4076e-01,\n",
      "         -1.1792e-02, -3.3888e-01, -4.6453e-01, -2.4497e-01, -8.5100e-01,\n",
      "          1.5499e-01,  9.2365e-01, -3.5049e-01,  1.8591e-01, -1.3304e-02,\n",
      "         -9.8248e-01,  4.0406e-01,  9.1249e-01,  6.4102e-02, -4.1410e-01,\n",
      "          8.5187e-01,  3.2068e-01,  8.6040e-01,  4.8130e-01, -1.6752e-01,\n",
      "         -2.1440e-01, -1.8284e-01,  9.5948e-01,  4.2199e-01,  8.6858e-01,\n",
      "          2.5981e-01, -1.7356e-01,  1.0339e-01,  9.7938e-01,  4.0198e-01,\n",
      "          9.1530e-01, -2.5387e-02, -4.5734e-01,  2.6814e-01,  8.4790e-01,\n",
      "          1.1150e-01,  9.6322e-01, -2.4447e-01, -7.9473e-01,  3.0759e-01,\n",
      "          5.2326e-01,  3.1312e-01,  9.4628e-01, -8.0689e-02,  3.2428e-01,\n",
      "          3.3256e-01,  5.2410e-02,  2.8323e-01,  3.1959e-01,  5.8752e-02,\n",
      "          1.4913e-01,  3.6951e-01,  1.8131e-02, -2.1775e-03, -1.2192e-02,\n",
      "          1.0653e-02,  2.6691e-03, -6.4404e-03,  2.7035e-02,  4.2313e-01,\n",
      "          3.2177e-01,  1.9855e-02,  9.9994e-01,  2.9063e-01, -7.2005e-02,\n",
      "          8.5206e-01,  3.4185e-01, -2.8867e-02,  9.2912e-01,  5.2060e-01,\n",
      "          4.0900e-02,  2.8650e-01,  3.1216e-01,  5.4764e-02,  1.9822e-01,\n",
      "          2.6948e-01,  6.8790e-02,  1.9414e-01,  2.7864e-01,  7.0205e-02,\n",
      "         -1.9515e-02,  3.1213e-01,  7.2150e-02, -6.2316e-02,  2.8696e-01,\n",
      "          1.7144e-01, -1.1263e-02,  2.6932e-01,  4.2155e-02,  1.3472e-02,\n",
      "          2.4523e-01, -1.3552e-01,  8.9752e-01, -1.6028e+00, -3.4697e-01,\n",
      "          1.5444e+00, -3.3773e+00, -5.4067e-01,  2.0559e+00, -4.0651e+00,\n",
      "         -4.2841e-01,  1.5820e-01,  2.9877e-01,  8.0978e-02,  2.9503e-01,\n",
      "          3.6074e-01,  1.3757e-01,  7.9166e-01,  7.1670e-01,  2.5252e-01,\n",
      "          1.6965e+00,  1.1688e+00,  3.6840e-01,  2.0955e+00,  1.4826e+00,\n",
      "          4.0402e-01,  2.8468e-01, -4.1181e-01,  1.0009e+00, -6.0793e-02,\n",
      "          3.0121e-01,  5.7089e-01, -9.8478e-01,  3.8238e-01,  4.0761e-01,\n",
      "         -5.4691e-02, -2.2573e-01,  9.5440e-02, -5.6817e-01,  6.1429e-01,\n",
      "          7.3810e-02,  6.0833e-02, -1.3696e+00,  8.4628e-01,  3.1127e-01,\n",
      "          4.8057e-01,  4.9646e-01,  4.4077e-01, -7.0236e-01,  1.3127e+00,\n",
      "          3.9526e-01, -8.7111e-01,  1.3215e+00,  4.1618e-01, -5.0412e-01,\n",
      "          1.0856e+00,  3.7397e-01,  6.0980e-02,  6.8315e-01, -1.2176e-01,\n",
      "         -8.4499e-01,  1.1724e+00,  2.4099e-01, -1.3254e+00, -2.3210e-01,\n",
      "         -2.3758e-01, -1.7030e+00,  5.0833e-03, -1.6751e+00, -8.7197e-01,\n",
      "         -1.1498e-01, -7.5715e+00, -3.3899e+00, -2.0279e+00, -8.1886e+00,\n",
      "         -2.8037e+00, -1.6646e+00, -7.0147e+00, -4.2592e+00,  5.8655e+00,\n",
      "         -9.0731e+00, -3.7276e+00,  5.6283e+00, -3.9880e-02, -1.2712e+00,\n",
      "          1.4884e+00,  8.0689e-01, -2.6026e+00,  4.5736e+00,  8.8375e-01,\n",
      "         -2.9839e+00,  4.7413e+00,  2.0723e+00, -3.2155e+00,  5.1109e+00,\n",
      "          1.7885e+00, -3.4509e+00,  5.0372e+00]])\n"
     ]
    }
   ],
   "source": [
    "print(type(observation))\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c5788a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(type(reward))\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c543d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bool'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(type(terminated))\n",
    "print(terminated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28db6b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bool'>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(type(truncated))\n",
    "print(truncated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8bb28b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'qpos': array([ 2.04939863e-01, -1.48138044e-01,  9.02875855e-01,  5.69595128e-01,\n",
      "        8.12703314e-01,  8.46006055e-02,  8.89800556e-02, -6.98218788e-01,\n",
      "        7.49476493e-02,  9.66407166e-04,  1.80792690e-01,  1.15077368e-01,\n",
      "        1.05973054e-02,  1.80115151e-01,  3.34181288e-01, -1.63914815e-02,\n",
      "       -2.11745392e-02,  8.68783532e-02,  7.75431692e-03, -2.04554534e-01,\n",
      "       -2.23168972e-02, -1.66907154e-01,  8.91311615e-02,  8.48458267e-02,\n",
      "       -8.24864300e-02, -2.75055398e-01, -1.65494532e-01,  7.95946600e-02,\n",
      "        1.09813856e-01, -7.68003545e-02,  7.85224809e-02, -2.41032977e-01,\n",
      "        1.16813197e-01, -1.52958112e-01,  1.70477761e-01, -1.30603084e-04,\n",
      "        1.84186332e-04, -2.48355583e-02,  5.35488345e-02, -4.84043163e-02,\n",
      "        1.89608907e-01,  1.22325826e-01,  7.71720487e-02, -1.14433942e-01,\n",
      "        2.40042734e-01,  2.50472267e-01, -1.20461346e-01,  6.48323439e-02,\n",
      "       -3.92148523e-01,  2.62719547e-01, -5.00806403e-01, -9.19044666e-01,\n",
      "       -3.10609411e-01,  2.76903454e-01, -1.00857190e-01, -1.30757264e+00,\n",
      "        2.24012176e-01,  9.14416032e-02,  6.59339397e-02, -4.84510358e-01,\n",
      "       -5.53045364e-01, -1.12579767e-01,  3.47240041e-01,  4.05918639e-01,\n",
      "       -2.49777983e-01,  7.05598790e-01,  1.24905106e+00,  7.31923464e-02,\n",
      "       -2.81601251e-01, -7.36826664e-02,  3.34087396e-01, -1.59875744e-01,\n",
      "       -3.18742917e-01, -2.57707324e-01,  4.21317319e-02, -4.72573351e-01]), 'qvel': array([ 4.04448231e-01, -2.28421693e-01,  5.24101146e-02, -3.58090890e-01,\n",
      "        1.05809770e+00, -6.73670020e-02,  7.22715376e-01, -3.09896106e-01,\n",
      "       -4.98191277e-01,  2.34464454e-01, -1.39449887e-03, -9.39477096e-01,\n",
      "       -1.04255053e+00, -3.01475109e-01,  8.62962366e-01,  9.87537006e-01,\n",
      "       -1.67884663e-02, -4.58239837e-02, -9.72078714e-01, -1.35447191e-01,\n",
      "       -1.90603280e-01,  1.86185181e+00, -7.18967191e-02,  2.44020068e-01,\n",
      "       -1.30914114e+00,  5.88000231e-01,  4.75326501e-02, -1.74272895e-01,\n",
      "       -8.73626452e-03, -3.22592299e-02, -1.00317564e-01,  1.02021908e-01,\n",
      "        1.24225573e-01,  6.01025908e-01, -3.30374041e-01,  1.12925120e-01,\n",
      "       -8.49284332e-01,  2.55487464e-01, -6.76627065e-01, -4.61718525e-01,\n",
      "       -1.20921399e+00,  8.70431710e-01, -9.74642684e-02,  4.14448443e-02,\n",
      "       -6.22712354e-01,  4.96829444e-01, -1.42882946e+00, -1.36770961e+00,\n",
      "       -2.40274644e+00, -4.39446570e+00, -5.73767739e+00, -1.12685556e-01,\n",
      "        5.82999761e-01, -6.79704716e-01, -6.87736643e+00,  8.83933184e-01,\n",
      "       -2.27389891e+00,  1.09727932e+00,  1.87128287e+00,  1.49243291e-01,\n",
      "       -4.59744959e-01,  2.57176718e-01, -3.11762415e-02, -8.57831857e-01,\n",
      "        3.40320046e+00,  5.58720188e-01,  1.34942385e-01,  2.42222471e-01,\n",
      "       -2.84777578e-01,  1.05593136e-01,  1.24873301e+00, -1.64128293e-01,\n",
      "        1.71618768e-02, -2.18273417e-01, -3.06363586e-01])}\n"
     ]
    }
   ],
   "source": [
    "print(type(info))\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05338016-0890-4f9e-8741-73985dbc89b3",
   "metadata": {},
   "source": [
    "### Computing Q-functions\n",
    "\n",
    "FB-CPR provides a way of directly computing the action-value function of any policy embedding `z` on any task embedding `z_r`. Then, the Q function of a policy $z$ is given by\n",
    "\n",
    "$Q(s,a, z) = F(s,a,z) \\cdot z_r$\n",
    "\n",
    "The task embedding can be computed in the following way. Given a set of samples labeled with rewards $(s,a,s',r)$, the task embedding is given by: \n",
    "\n",
    "$z_r = \\mathrm{normalised}(\\sum_{i \\in \\mathrm{batch}} r_i B(s'_i))$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98e3b022-22d1-4602-ab84-b056d621b702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([255.3591, 200.0598, 198.1490, 198.0176, 198.7776, 212.7530, 197.3234,\n",
      "        219.2996, 223.4091, 198.2895])\n"
     ]
    }
   ],
   "source": [
    "def Qfunction(state, action, z_reward, z_policy):\n",
    "    F = model.forward_map(obs=state, z=z_policy.repeat(state.shape[0],1), action=action) # num_parallel x num_samples x z_dim\n",
    "    Q = F @ z_reward.ravel()\n",
    "    return Q.mean(axis=0)\n",
    "\n",
    "z_reward = model.sample_z(1)\n",
    "z_policy = model.sample_z(1)\n",
    "state = torch.rand((10, env.observation_space.shape[0]), device=model.cfg.device, dtype=torch.float32)\n",
    "action = torch.rand((10, env.action_space.shape[0]), device=model.cfg.device, dtype=torch.float32)*2 - 1\n",
    "Q = Qfunction(state, action, z_reward, z_policy)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9072da84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea76e576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f168c33-a35f-4335-bc97-e6d4eeb5fce5",
   "metadata": {},
   "source": [
    "## Prompting the model\n",
    "\n",
    "We have seen that we can condition the model via the context variable `z`. We can control the task to execute via _prompting_ (or _policy inference_)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10d2b486-63c1-45f7-bfca-acd5308da1ec",
   "metadata": {},
   "source": [
    "### Reward prompts\n",
    "The first version of inference we investigate is the reward prompting, i.e., given a set of reward label samples we can infer in a zero-shot way the near-optimal policy for solving such task.\n",
    "\n",
    "First step, download the data for inference. We provide a buffer for inference of about 500k samples. This buffer has been generated by randomly subsampling the final replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26312a39-fdb8-4843-a3e2-08f0dafc3db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_dir = \"metamotivo-S-1-datasets\"\n",
    "dataset = \"buffer_inference_500000.hdf5\"\n",
    "buffer_path = hf_hub_download(\n",
    "        repo_id=\"facebook/metamotivo-S-1\",\n",
    "        filename=f\"data/{dataset}\",\n",
    "        repo_type=\"model\",\n",
    "        local_dir=local_dir,\n",
    "    )\n",
    "print(buffer_path)\n",
    "\n",
    "hf = h5py.File(buffer_path, \"r\")\n",
    "print(hf.keys())\n",
    "data = {}\n",
    "for k, v in hf.items():\n",
    "    print(f\"{k:20s}: {v.shape}\")\n",
    "    data[k] = v[:]\n",
    "buffer = DictBuffer(capacity=data[\"qpos\"].shape[0], device=\"cpu\")\n",
    "buffer.extend(data)\n",
    "del data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ac150fc7-0719-4428-8ef1-eac48ddf0d9a",
   "metadata": {},
   "source": [
    "Now that we have download the h5 file for inference, we can conveniently loaded it in a buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af64888e-c992-4c0b-aa7b-9421942ee605",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = buffer.sample(5)\n",
    "for k, v in batch.items():\n",
    "    print(f\"{k:20s}: {v.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f1238a0-1ffc-4de9-a698-7853ea7fdd92",
   "metadata": {},
   "source": [
    "As you can see, the buffer does not provide a reward signal. We need to label this buffer with the desired reward function. We provide API for that but here we start looking into the basic steps:\n",
    "* Instantiate a reward function\n",
    "* Computing the reward from the batch data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedac7ea-6365-41ce-b7cb-6b2f1ccbd4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_fn = humenv_rewards.LocomotionReward(move_speed=2.0) # move ahead with speed 2\n",
    "# humenv provides also a name-base reward initialization. We could\n",
    "# get the same reward function in this way\n",
    "reward_fn = make_from_name(\"move-ego-0-2\") \n",
    "print(reward_fn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d95db165-5c5c-402c-9ddf-6cfed05f48a8",
   "metadata": {},
   "source": [
    "We can call the method `__call__` to obtain a reward value from the physics state. This function receives a mujoco model, qpos, qvel and the action. See the humenv tutorial for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa8a3dd-159e-45a9-95b7-c52d6244b6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100_000\n",
    "batch = buffer.sample(N)\n",
    "rewards = []\n",
    "for i in range(N):\n",
    "    rewards.append(\n",
    "        reward_fn(\n",
    "            env.unwrapped.model,\n",
    "            qpos=batch[\"next_qpos\"][i],\n",
    "            qvel=batch[\"next_qvel\"][i],\n",
    "            ctrl=batch[\"action\"][i])\n",
    "    )\n",
    "rewards = np.stack(rewards).reshape(-1,1)\n",
    "print(rewards.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ccc931-9be2-4a88-b66f-70d369648c2c",
   "metadata": {},
   "source": [
    "**Note** that the reward functions implemented in humenv are functions of next state and action which means we need to use `next_qpos` and `next_qvel` that are the physical state of the system at the next state.\n",
    "\n",
    "We provide a multi-thread version for faster relabeling, see `metamotivo.wrappers.humenvbench.relabel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ca4068-e529-43d4-a110-5f224d118d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metamotivo.wrappers.humenvbench import relabel\n",
    "rewards = relabel(\n",
    "    env,\n",
    "    qpos=batch[\"next_qpos\"],\n",
    "    qvel=batch[\"next_qvel\"],\n",
    "    action=batch[\"action\"],\n",
    "    reward_fn=reward_fn, \n",
    "    max_workers=8\n",
    ")\n",
    "print(rewards.ravel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8327f700-746f-4d76-b0e2-4b66cd4b44de",
   "metadata": {},
   "source": [
    "We can now infer the context `z` for the selected task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5537121c-ce60-44e0-9fdf-5457bcad6c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.reward_wr_inference(\n",
    "    next_obs=batch[\"next_observation\"],\n",
    "    reward=torch.tensor(rewards, device=model.cfg.device, dtype=torch.float32)\n",
    ")\n",
    "print(z.shape)\n",
    "\n",
    "observation, _ = env.reset()\n",
    "frames = [env.render()]\n",
    "for i in range(10):\n",
    "    action = model.act(observation, z, mean=True)\n",
    "    observation, reward, terminated, truncated, info = env.step(action.cpu().numpy().ravel())\n",
    "    frames.append(env.render())\n",
    "\n",
    "media.show_video(frames, fps=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bd2550c-06a5-4353-b3f5-4eeb56c30d70",
   "metadata": {},
   "source": [
    "Let's compute the **Q-function** for this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3719a3bf-8282-430a-84a6-c5067a4cfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_reward = torch.sum(\n",
    "    model.backward_map(obs=batch[\"next_observation\"]) * torch.tensor(rewards, dtype=torch.float32, device=model.cfg.device),\n",
    "    dim=0\n",
    ")\n",
    "z_reward = model.project_z(z_reward)\n",
    "Q = Qfunction(batch[\"observation\"], batch[\"action\"], z_reward, z)\n",
    "print(Q)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "42121e99-c920-4d69-a499-ec039e0b8e05",
   "metadata": {},
   "source": [
    "# Goal and Tracking prompts\n",
    "The model supports two other modalities, `goal` and `tracking`. These two modalities expose similar functions for context inference:\n",
    "- `def goal_inference(self, next_obs: torch.Tensor) -> torch.Tensor`\n",
    "- `def tracking_inference(self, next_obs: torch.Tensor) -> torch.Tensor`\n",
    "  \n",
    "We show an example on how to perform goal inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1de2e-d792-446c-ba1f-7b07abf69cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_qpos = np.array([0.13769039,-0.20029453,0.42305034,0.21707786,0.94573617,0.23868944\n",
    ",0.03856998,-1.05566834,-0.12680767,0.11718296,1.89464102,-0.01371153\n",
    ",-0.07981451,-0.70497424,-0.0478,-0.05700732,-0.05363342,-0.0657329\n",
    ",0.08163511,-1.06263979,0.09788937,-0.22008936,1.85898192,0.08773695\n",
    ",0.06200327,-0.3802791,0.07829525,0.06707749,0.14137152,0.08834448\n",
    ",-0.07649805,0.78328658,0.12580912,-0.01076061,-0.35937259,-0.13176489\n",
    ",0.07497022,-0.2331914,-0.11682692,0.04782308,-0.13571422,0.22827948\n",
    ",-0.23456622,-0.12406075,-0.04466465,0.2311667,-0.12232673,-0.25614032\n",
    ",-0.36237662,0.11197906,-0.08259534,-0.634934,-0.30822742,-0.93798716\n",
    ",0.08848668,0.4083417,-0.30910404,0.40950143,0.30815359,0.03266103\n",
    ",1.03959336,-0.19865537,0.25149713,0.3277561,0.16943092,0.69125975\n",
    ",0.21721349,-0.30871948,0.88890484,-0.08884043,0.38474549,0.30884107\n",
    ",-0.40933304,0.30889523,-0.29562966,-0.6271498])\n",
    "env.unwrapped.set_physics(qpos=goal_qpos, qvel=np.zeros(75))\n",
    "goal_obs = torch.tensor(env.unwrapped.get_obs()[\"proprio\"].reshape(1,-1), device=model.cfg.device, dtype=torch.float32)\n",
    "print(\"goal pose\")\n",
    "media.show_image(env.render())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24755455-28f9-41ad-b7e6-26f5b10ae13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.goal_inference(next_obs=goal_obs)\n",
    "\n",
    "\n",
    "observation, _ = env.reset()\n",
    "frames = [env.render()]\n",
    "for i in range(10):\n",
    "    action = model.act(observation, z, mean=True)\n",
    "    observation, reward, terminated, truncated, info = env.step(action.cpu().numpy().ravel())\n",
    "    frames.append(env.render())\n",
    "\n",
    "media.show_video(frames, fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
